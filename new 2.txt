from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_dir = "/cloudfiles/code/Users/Dooman.Akbarian/LLM/Llama-3.1-8B"  # your path

tokenizer = AutoTokenizer.from_pretrained(
    model_dir,
    local_files_only=True,
)

dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    torch_dtype=dtype,
    local_files_only=True,
)

model.to(device)

prompt = "You are a helpful assistant. Answer briefly.\nUser: What is machine learning?\nAssistant:"
inputs = tokenizer(prompt, return_tensors="pt").to(device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=128,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
    )

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
