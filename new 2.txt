pip install pandas textstat scikit-learn openpyxl


import pandas as pd
from textstat import flesch_reading_ease, flesch_kincaid_grade
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

# ----------- METRIC FUNCTIONS -----------

def get_readability_metrics(text):
    return {
        "flesch_reading_ease": flesch_reading_ease(text),
        "flesch_kincaid_grade": flesch_kincaid_grade(text)
    }

def get_length_ratio(original, summary):
    try:
        return len(summary.split()) / len(original.split()) if len(original.split()) > 0 else 0
    except:
        return 0

def get_repetition_score(text):
    words = text.split()
    total = len(words)
    unique = len(set(words))
    return 1 - (unique / total) if total > 0 else 0

def get_ngram_repetition(text, n=3):
    words = text.split()
    if len(words) < n:
        return 0
    ngrams = zip(*[words[i:] for i in range(n)])
    ngram_list = [' '.join(ng) for ng in ngrams]
    return len(ngram_list) - len(set(ngram_list))

def keyword_coverage(original, summary, top_k=10):
    try:
        vectorizer = CountVectorizer(stop_words='english', max_features=top_k)
        X = vectorizer.fit_transform([original])
        keywords = set(vectorizer.get_feature_names_out())
        return len([k for k in keywords if k in summary.lower()]) / len(keywords) if keywords else 0
    except:
        return 0

# ----------- MAIN PIPELINE -----------

def evaluate_llm_outputs(df):
    results = []

    for index, row in df.iterrows():
        original = str(row['V_COMMENTS'])
        brief = str(row['Brief Overview'])
        desc = str(row['Description'])

        # Brief Overview metrics
        brief_read = get_readability_metrics(brief)
        brief_metrics = {
            "brief_length_ratio": get_length_ratio(original, brief),
            "brief_repetition_score": get_repetition_score(brief),
            "brief_ngram_repetition": get_ngram_repetition(brief),
            "brief_keyword_coverage": keyword_coverage(original, brief),
            "brief_reading_ease": brief_read['flesch_reading_ease'],
            "brief_kincaid_grade": brief_read['flesch_kincaid_grade'],
        }

        # Description metrics
        desc_read = get_readability_metrics(desc)
        desc_metrics = {
            "desc_length_ratio": get_length_ratio(original, desc),
            "desc_repetition_score": get_repetition_score(desc),
            "desc_ngram_repetition": get_ngram_repetition(desc),
            "desc_keyword_coverage": keyword_coverage(original, desc),
            "desc_reading_ease": desc_read['flesch_reading_ease'],
            "desc_kincaid_grade": desc_read['flesch_kincaid_grade'],
        }

        # Combine all
        all_metrics = {**brief_metrics, **desc_metrics}
        results.append(all_metrics)

    metric_df = pd.DataFrame(results)
    df_with_metrics = pd.concat([df, metric_df], axis=1)

    # Aggregate stats
    averages = metric_df.mean().to_frame(name="Average Score")

    return df_with_metrics, averages


# Load your dataset
df = pd.read_excel("your_file.xlsx")  # or pd.read_csv("your_file.csv")

# Evaluate
evaluated_df, average_metrics = evaluate_llm_outputs(df)

# Save results
evaluated_df.to_excel("evaluated_llm_outputs.xlsx", index=False)
average_metrics.to_excel("summary_metrics.xlsx")


